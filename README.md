# NBAdrafter
This is my personal project for the Machine Learning course at NYU Tandon School of Engineering.\
*Performed by Dutchak Bohdan* :sleeping:
<br><br>





## Description
Here will be description of the project generally, also pipelines, technologies explained etc etc.\
Some of the parts will be written on Python3. The reasons of it is the variety of different libraries and tools it has to collect, process and plot data. Also I feel more confident with this language.
<details>
  <summary>
    Expand
  </summary>

**Soon...**
</details>
<br>





## Weekly progress
I like to keep my progress together. Also, since I messed up with deadlines, I decided to handle my progress here. Also, it can be formatted into a fancy page.\
Current progress: **[Week 1](https://github.com/bohdan-dutchak/NBAdrafter/blob/main/README.md#week-2)**.

<details>
  <summary>
    Show progress
  </summary>



<details>
<summary><h3>Week 1</h3></summary>
  This one is the pilot week, I will try to cope with everything.    
  
#### Work done
  - 1-2 chptrs ISLR
  - Watched the lecture
  - Introduction to ML
  - [Cool vid about bias and variance](https://www.youtube.com/watch?v=EuBBz3bI-aA)
#### Problems faced
  - None actually, but it is just a beggining
#### Next steps
  - Complete first 4 weeks and start the project.

#### Brief list on notes for this week 
  - There are two paradigms of estimation of the model:
    1. Prediction (focusing on the result i.e. the output variable)
    2. Inference (providing analytics of the different trends and relations between variables)
  - Regarding to the paradigms, there is a trade-off between more flexible (Deep learning, SVM, Boosting, Bagging, GAM) and more interpretable models (Lasso, OLS). The more flexible model is, the bigger *variance* it has and vice versa with *bias*.
  - Learning of the model can be supervised or unsupervised (more rarelly semi-supervised), which depends on the existence of the response variable.
  - Very low MSE on training data may indicate overfitting.
  - bias-variance trade-off is an estimation method of test MSE by result train variable.
  - KNN is the model with optionally chosen K - the number of nearest neighbors. The smaller K is, the more flexible model.
</details>



<details>
<summary><h3>Week 2</h3></summary>
  Empty yet
  
#### Work done
  - [Confounding explained](https://www.youtube.com/watch?v=bcfg9kcxeuU)
#### Problems faced
  - In chapter 2 it is said, that it is hard to compute model accuracy on the testing data, since sometimes there is no test data. Why can't we just take a 30% of it as a data for testing. So we don't train model on this part.
  - 
#### Next steps
  - 

#### Brief list on notes for this week   
  - You can use OLS to make classifier with 2 classes, but it is not recommended
  - In order to decrease risks of false positive we can manually decrease the threshold of the probability from 0.5 to i.e. 0.1
  - Confounding is a bias that leads to the wrong relation dependencies between some variable and result. Here we have example of probability of default given the person is student or not. Confounding misleads us to the conclusion that students are more risky creditee, but those are so only when the credit balance is unknown. Otherwise non-student is more risky.
  - 
</details>



<details>
<summary><h3>Week 3</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>



<details>
<summary><h3>Week 4</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  - 
#### Something else
  -      
</details>



<details>
<summary><h3>Week 5</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>
</details>
:blue_square::blue_square::blue_square::blue_square:<br>
:yellow_square::yellow_square::yellow_square::yellow_square:
