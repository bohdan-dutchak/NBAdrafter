# NBAdrafter
This is my personal project for the Machine Learning course at NYU Tandon School of Engineering.\
*Performed by Dutchak Bohdan* :sleeping:
<br><br>





## Description
Here will be description of the project generally, also pipelines, technologies explained etc etc.\
Some of the parts will be written on Python3. The reasons of it is the variety of different libraries and tools it has to collect, process and plot data. Also I feel more confident with this language.
<details>
  <summary>
    Expand
  </summary>

**Soon...**
</details>
<br>





## Weekly progress
I like to keep my progress together. Also, since I messed up with deadlines, I decided to handle my progress here. Also, it can be formatted into a fancy page.\
Current progress: **[Week 1](https://github.com/bohdan-dutchak/NBAdrafter/blob/main/README.md#week-1)**.

<details>
  <summary>
    Show progress
  </summary>



<details>
<summary><h3>Week 1</h3></summary>
  This one is the pilot week, I will try to cope with everything.    
  
#### Work done
  - 1-2 chptrs ISLR
  - Watched the lecture
  - Introduction to ML
#### Problems faced
  - None actually, but it is just a beggining
#### Next steps
  - Complete first 4 weeks and start the project.

#### Brief list on notes for this week 
  - There are two paradigms of estimation of the model:
    1. Prediction (focusing on the result i.e. the output variable)
    2. Inference (providing analytics of the different trends and relations between variables)
  - Regarding to the paradigms, there is a trade-off between more flexible (Deep learning, SVM, Boosting, Bagging, GAM) and more interpretable models (Lasso, OLS). The more flexible model is, the bigger *variance* it has and vice versa with *bias*.
  - Learning of the model can be supervised or unsupervised (more rarelly semi-supervised), which depends on the existence of the response variable.
  - Very low MSE on training data may indicate overfitting
  - bias-variance trade-off is an estimation method of test MSE by result train variable.
  - KNN is the model with optionally chosen K - the number of nearest neighbors. The smaller K is, the more flexible model.
</details>



<details>
<summary><h3>Week 2</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>



<details>
<summary><h3>Week 3</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>



<details>
<summary><h3>Week 4</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>



<details>
<summary><h3>Week 5</h3></summary>
  Empty yet
  
#### Work done
  - 
#### Problems faced
  -
#### Something else
  -      
</details>
</details>
:blue_square::blue_square::blue_square::blue_square:<br>
:yellow_square::yellow_square::yellow_square::yellow_square:
